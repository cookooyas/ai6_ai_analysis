{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7538e96",
   "metadata": {},
   "source": [
    "# Pose Estimation Models\n",
    "****\n",
    "#### 주제에 부합한 모델을 찾고 각 모델을 이용한 후처리에 중점을 두었습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ccfa40",
   "metadata": {},
   "source": [
    "## 관점\n",
    "\n",
    "#### 1. 모델이 person pose를 정확히 detecting할 수 있는가?\n",
    "- 댄스를 측정하기 위해서 사람을 정확히 인지하는 것은 매우 중요한 문제입니다.\n",
    "- 탑다운/바텀업 등 여러 방법을 통해 person을 정확히 detecting 해야합니다.\n",
    "- 당연히 처리 속도의 관점에서는 바텀업방식이 좋겠지만 여러 후처리를 위해서는 정확하게 detecting하는것도 중요해보인다.\n",
    "\n",
    "#### 2. 모델이 pose estimation을 잘 할 수 있는가?\n",
    "- 모델들은 2D/3D등 여러 방법으로 detecting한 person의 pose를 estimating합니다.\n",
    "  또한 관절 좌표를 잘 catch하는것도 이번 프로젝트 주제에 있어 중요한 요소입니다.\n",
    "- pose estimation또한 single person pose estimation, 또는 multi person pose estimation의 여부에 따라 프로젝트의 확장성에도 영향을 줍니다.\n",
    "\n",
    "\n",
    "#### 3. 모델이 6fps이상의 realtime을 유지할 수 있는가?\n",
    "- 모델들은 다양한 방식으로 pose estimating을 진행합니다. 이에따라 realtime을 준수하지 못하는 문제가 발생할 수 있습니다. 이를 해결하기위해 realtime또한 중요한 요소라고 할 수 있겠습니다.\n",
    "- 또한 이번 프로젝트의 VM환경은 gpu를 지원하지 않는다고 들었기에, cpu에서도 연산할 수 있는 적절한 처리 속도를 요구합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc3ee26",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b505ffb5",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe6e0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow opencv-python \n",
    "# # https://pysource.com/2019/07/08/yolo-real-time-detection-on-cpu/ 에서 tiny-yolov3 weights,cfg를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2772aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07637c3",
   "metadata": {},
   "source": [
    "## [TFLite (movenet/lightning)](https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3)\n",
    "#### 요약\n",
    "- single pose detection을 cpu환경에서도 매우 빠르게 잡아낸다.\n",
    "- 17개 주요 keypoints를 2D coordinate와 confidence로 반환한다.\n",
    "- 생각보다는 흔들림이 심하고 single pose estimation의 특성상 여러사람이 인식되면 많이 불안정해진다\n",
    "- 입력값이 제한되어있다.(192x192x3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c84bbdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "# https://tfhub.dev/google/lite-model/movenet/singlepose/lightning/3에서 tflite를 다운받아온다.\n",
    "path=\"put/your/model/path/here\"\n",
    "interpreter = tf.lite.Interpreter(model_path=path)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd14a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "\n",
    "# draw_keypoints()는 각 프레임별로 받아온 키포인트중 confidence가 일정 수준 이상인\n",
    "# 키포인트의 위치좌표에 원을 그려준다.\n",
    "# isMatch의 여부에 따라 다른 색상의 원을 그려주게끔 정의했다.\n",
    "\n",
    "def draw_keypoints(frame,keypoints,confidence,isMatch):\n",
    "    y,x,c=frame.shape\n",
    "    #np.squeeze는 차원을 줄여준다\n",
    "    shaped=np.squeeze(np.multiply(keypoints,[y,x,1]))\n",
    "    color = (0,255,0) if isMatch else (255,0,0)\n",
    "    for kp in shaped:\n",
    "        ky,kx,kp_conf=kp\n",
    "        if kp_conf > confidence:\n",
    "            cv2.circle(frame,(int(kx),int(ky)),4,color,-1)\n",
    "            # -1은 원을 해당색상으로 채우는걸 의미한다.\n",
    "\n",
    "# draw_connections()는 각 프레임별로 받아오는 키포인트 중 confidence가 일정 수준 이상인\n",
    "# 키포인트의 위치좌표들에 대하여 정의된 edge에 맞게끔 선을그어 연결된 뼈대가 보여지게 한다.\n",
    "# isMatch의 여부에 따라 다른 색상의 선을 그려주게끔 정의했다.\n",
    "            \n",
    "def draw_connections(frame,keypoints,edges,confidence,isMatch):\n",
    "    y,x,c=frame.shape\n",
    "    #np.squeeze는 차원을 줄여준다\n",
    "    shaped=np.squeeze(np.multiply(keypoints,[y,x,1]))\n",
    "    color = (0,255,0) if isMatch else (255,0,0)\n",
    "    for edge,_ in edges.items():\n",
    "        p1,p2 = edge\n",
    "        y1,x1,c1=shaped[p1]\n",
    "        y2,x2,c2=shaped[p2]\n",
    "        if(c1>confidence)&(c2>confidence):\n",
    "            cv2.line(frame,(int(x1),int(y1)),(int(x2),int(y2)),color,2)\n",
    "            #2는 선 굵기를 의미한다.\n",
    "   \n",
    "\n",
    "# cosine_sim()은 테스트 좌표와 정답 좌표의 코사인유사도를 검증하고\n",
    "# 해당 유사도와 similarity를 비교해 isMatch의 boolean을 결정한다.\n",
    "# 간단한 측정을 위하여 존재하는 edge들을 벡터로 생각하여\n",
    "# 모든 벡터가 similarity를 초과하면 True를 반환하게끔 정의하였다\n",
    "def cosine_sim(my_keypoint,q_keypoint,confidence,similarity):\n",
    "    my_shape=np.squeeze(my_keypoints)\n",
    "    q_shape=np.squeeze(q_keypoints)\n",
    "\n",
    "    result={}\n",
    "    for edge,_ in EDGES.items():\n",
    "        p1,p2=edge\n",
    "        qy1,qx1,qc1=q_shape[p1]\n",
    "        qy2,qx2,qc2=q_shape[p2]\n",
    "        a=[qx2-qx1,qy2-qy1]\n",
    "\n",
    "        my1,mx1,mc1=my_shape[p1]\n",
    "        my2,mx2,mc2=my_shape[p2]\n",
    "        b=[mx2-mx1,my2-my1]\n",
    "        if (qc1>confidence) & (qc2>confidence):\n",
    "            sim=dot(a,b)/(norm(a)*norm(b))\n",
    "            if sim<similarity:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "# make_prediction()은 준비한 모델에 맞게 입력값을 전처리하고,\n",
    "# invoke를 통해 얻은 결과를 반환해준다.\n",
    "# 결과 : 각 키포인트를 키로 하는  y , x 좌표값과 confidence 이 담긴 딕셔너리\n",
    "\n",
    "def make_prediction(img):\n",
    "    #reshaping img\n",
    "    img=tf.image.resize_with_pad(np.expand_dims(img,axis=0),192,192)\n",
    "    input_image=tf.cast(img,dtype=tf.float32)\n",
    "    \n",
    "    #setup in/out put\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details=interpreter.get_output_details()\n",
    "    \n",
    "    #make predictions\n",
    "    interpreter.set_tensor(input_details[0]['index'],np.array(input_image))\n",
    "    interpreter.invoke()\n",
    "    keypoints_with_scores=interpreter.get_tensor(output_details[0]['index'])\n",
    "    #interpreter를 통해 invoke된 결과가 keypoints_with_scores에 업데이트된다.\n",
    "    \n",
    "    return keypoints_with_scores\n",
    "\n",
    "# testing()은 단순히 사진을 테스트해보기위한 용도이며, 해당 사진과 같은 포즈를 취할 때\n",
    "# isMatch가 true가 되게끔 정의했었다. 다만 쓸일은 거의 없을것이다.\n",
    "# 정답 키포인트를 반환한다고 생각하면 되겠다.\n",
    "\n",
    "def testing(img_path='./Desktop/question2.jpg'):\n",
    "    q_img=cv2.imread(img_path)\n",
    "    q_keypoints=make_prediction(q_img)\n",
    "    return q_img,q_keypoints\n",
    "\n",
    "# thug_life()는 이펙트 확인하기 위해서 정의한 함수이다.\n",
    "# confidence가 일정 수준 이상인 눈의 좌표를 가져와서 해당 좌표에 맞게끔 이미지를 수정하고\n",
    "# 최종적으로 프레임에 해당 이미지가 반영되게 해주는 함수이다.\n",
    "\n",
    "def thug_life(frame,keypoint,confidence,img_path=\"./Desktop/thug.png\",ratio=3):\n",
    "    y,x,c=frame.shape\n",
    "    #np.squeeze는 차원을 줄여준다\n",
    "    shaped=np.squeeze(np.multiply(keypoints,[y,x,1]))\n",
    "    y0,x0,c0=shaped[0]\n",
    "    y1,x1,c1=shaped[1]\n",
    "    y2,x2,c2=shaped[2]\n",
    "    if (c1>confidence)&(c2>confidence)&(c0>confidence):\n",
    "        mask = cv2.imread(img_path, cv2.IMREAD_UNCHANGED)\n",
    "        \n",
    "        #*3은 이미지 자체의 크기가 작기에 눈과 눈사이의 거리보다 3배 늘려준것. 수정가능\n",
    "        resize_x=int(norm([abs(x1-x2),abs(y1-y2)])*ratio)\n",
    "        resize_y=int(mask.shape[0]*(resize_x/mask.shape[1]))\n",
    "        center_point=(int((x1+x2-resize_x)/2),int((y1+y2-resize_y)/2))\n",
    "        \n",
    "        mask=cv2.resize(mask,(resize_x,resize_y))\n",
    "        #src, mask, dst는 사이즈가 다 같아야하고, src, dst는 타입도 같아야한다.\n",
    "        #컬러면 컬러, 그레이스케일이면 그레이스케일로 맞춰주고, mask는 무조건 그레이스케일이여야 한다.\n",
    "        mask_grey=mask[:,:,3]\n",
    "        mask_color=mask[:,:,:-1]\n",
    "        h,w=mask_grey.shape[:2]\n",
    "        \n",
    "        #+20은 위치 조절을 위해서 끼워넣었다.\n",
    "        crop=frame[center_point[1]:center_point[1]+h,center_point[0]:center_point[0]+w]\n",
    "#         print(mask_color.shape,\"\\n\",mask_grey.shape,'\\n',crop.shape)\n",
    "        cv2.copyTo(mask_color,mask_grey,crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8659c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가장 기본적인 cv2 영상 캡쳐 형식이다.\n",
    "# cv2에서 지원하는 VideoCapture()를 통해 원하는 영상 path를 넣어주거나\n",
    "# 0을 넣어 내 로컬 기기의 카메라에 접근할 수 있다.\n",
    "# 위에서 구현해둔 코드들을 대입하여 여러기능을 확인할 수 있겠다.\n",
    "# q를 눌리거나 영상이 종료되면 창이 닫힌다.\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "isMatch=False\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    if ret:\n",
    "        img=frame.copy()\n",
    "        keypoints=make_prediction(img)\n",
    "        #rendering\n",
    "        confidence_threshold=0.3\n",
    "#         thug_life(frame,keypoints,confidence_threshold)\n",
    "#         draw_keypoints(frame,keypoints,confidence_threshold,isMatch)\n",
    "        cv2.imshow('MoveNet Lightning',frame)\n",
    "        if cv2.waitKey(10)&0xFF==ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release() #캠끄기\n",
    "cv2.destroyAllWindows() #창닫기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c387c2ca",
   "metadata": {},
   "source": [
    "### 예시영상 pose estimation 결과\n",
    "#### 당연한 이야기지만 겹쳐지는 순간 single pose estimation에 noise가 크게 발생한다\n",
    "<img width=\"80%\" src=\"./img/output1.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb73655f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 아래 코드는 임시로 segmentation을 위해 구현해보고 있었던 함수이다.\n",
    "# 도저히 해당 방법으로는 깔끔하게 segmentation을 진행할 수 없었다..\n",
    "# 해당 코드에서 Canny를 통해 후처리 이펙트를 넣을 수도 있을것같다는 생각을 했다.\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "cap = cv2.VideoCapture('./Desktop/4.mp4')\n",
    "fps = cap.get(cv2.CAP_PROP_FPS) # 프레임 수 구하기\n",
    "dst=cv2.imread('./Desktop/32.jpeg')\n",
    "\n",
    "isMatch=True\n",
    "confidence_threshold=0.4\n",
    "delay = int(1000/fps)\n",
    "# 배경 제거 객체 생성 --- ①\n",
    "\n",
    "knnSubtractor = cv2.createBackgroundSubtractorKNN(100, 32 ,False)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(20,20))\n",
    "kernel_sharpening=np.array([[-1,-1,-1],[-1,9,-1],[-1,-1,-1]])\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    # 배경 제거 마스크 계산 --- ②\n",
    "    \n",
    "    img=frame.copy()\n",
    "    keypoints=make_prediction(img)\n",
    "    dst=cv2.resize(dst,(frame.shape[1],frame.shape[0]))\n",
    "#     draw_keypoints(frame,keypoints,confidence_threshold,isMatch)\n",
    "    gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "#     hsv=cv2.cvtColor(frame,cv2.COLOR_BGR2HSV)\n",
    "#     rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "#     yuv=cv2.cvtColor(frame,cv2.COLOR_BGR2YUV)\n",
    "#     lab=cv2.cvtColor(frame,cv2.COLOR_BGR2LAB)\n",
    "#     lab2hsv=cv2.cvtColor(lab,cv2.COLOR_BGR2HSV)\n",
    "#     hsv2lab=cv2.cvtColor(hsv,cv2.COLOR_BGR2LAB)\n",
    "#     rgb2hsv=cv2.cvtColor(rgb,cv2.COLOR_BGR2HSV)\n",
    "    \n",
    "#     blured=cv2.filter2D(frame,-1,kernel_sharpening)\n",
    "#     blur2rgb=cv2.cvtColor(blured,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#     sharpened=cv2.filter2D(frame,-1,kernel_sharpening)\n",
    "#     sharpened=cv2.filter2D(sharpened,-1,kernel_sharpening)\n",
    "    blured=cv2.blur(gray,(3,3))\n",
    "#     sharpened=cv2.filter2D(blured,-1,kernel_sharpening)\n",
    "    edge=cv2.Canny(blured,300,500)\n",
    "#     edge=cv2.cvtColor(edge,cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "#     edge=cv2.blur(edge,(7,7))\n",
    "    background_extraction_mask = fgbg.apply(gray)\n",
    "#     background_extraction_mask = cv2.morphologyEx(background_extraction_mask, cv2.MORPH_OPEN, kernel)\n",
    "#     background_extraction_mask = cv2.morphologyEx(background_extraction_mask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    #     background_extraction_mask = cv2.morphologyEx(background_extraction_mask,cv2.MORPH_CLOSE, kernel)\n",
    "    background_extraction_mask = cv2.dilate(background_extraction_mask,kernel,iterations=4)\n",
    "\n",
    "    \n",
    "    \n",
    "    background_extraction_mask = np.stack((background_extraction_mask,)*3, axis=-1)\n",
    "#     draw_keypoints(frame,keypoints,confidence_threshold,isMatch)\n",
    "#     draw_connections(frame,keypoints,EDGES,confidence_threshold,isMatch)\n",
    "\n",
    "    \n",
    "    bitwise_image = cv2.bitwise_and(frame, background_extraction_mask)\n",
    "\n",
    "    concat_image = np.concatenate((frame,bitwise_image), axis=1)\n",
    "#     concat_image = np.concatenate((frame,background_extraction_mask), axis=1)\n",
    "    \n",
    "#     print(frame.shape,\"\\n\",fgmask.shape,'\\n',dst.shape)\n",
    "#     cv2.copyTo(frame,fgmask,dst)\n",
    "    cv2.imshow('background extraction video', concat_image)\n",
    "#     cv2.imshow('back',back)\n",
    "#     cv2.imshow('fgmask',fgmask)\n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1089c51e",
   "metadata": {},
   "source": [
    "의외의 발견 cv2.Canny\n",
    "\n",
    "![ex_screenshot](./img/Canny.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4e60d8",
   "metadata": {},
   "source": [
    "## [Mediapipe-MediaPose](https://developers.google.com/mediapipe/solutions/vision/pose_landmarker)\n",
    "#### 요약\n",
    "- 탑다운 방식임에도 불구하고 cpu환경에서 빠른 연산을 보여준다 (12~14fps)\n",
    "- single pose estimating을 진행하여 32개 주요 Landmark를 반환한다.\n",
    "- pose estimating을 할 때 가려지는 부분들에 대한 예측이 이루어지고 여러사람이 겹치지지 않는 이상 지속적으로 추적을 진행하여 부드러운 movement가 인상적이다.\n",
    "- 객체가 작아져도 Landmark를 꾸준히 탐색하는 등 single pose estimating을 굉장히 잘 잡아낸다\n",
    "- 탑다운 방식이다 보니 movenet보다는 프레임드랍이 심하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "526e7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6063260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "# fps_detector()은 프레임의 고정 좌표에 fps값을 표시해주며,\n",
    "# pTime과 cTime이 꾸준하게 간격을 벌려주게된다.\n",
    "# 하나의 프레임이 나오는데 걸리는시간의 역수로 초당 프레임(fps)를 측정한다.\n",
    "\n",
    "def fps_detector(frame,pTime):\n",
    "    cTime=time.time()\n",
    "    fps=1/(cTime-pTime)\n",
    "    pTime=cTime\n",
    "    cv2.putText(frame,str(int(fps)),(70,50),cv2.FONT_HERSHEY_PLAIN,3,(0,255,0),3)\n",
    "    return pTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d566f59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mediapipe의 pose.Pose()모델을 가져오고 이후의 연산에서 입력한 이미지의 process를 진행하여\n",
    "#landmark나 파라미터의 여부에 따라 segmentation_mask정보를 가져오는등의 여부를 result로 반환한다.\n",
    "mpPose=mp.solutions.pose\n",
    "pose=mpPose.Pose()\n",
    "\n",
    "#mediapipe의 soulution.drawing_utils를 이용해 다양한 수정이 가능하다\n",
    "mpDraw=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f56339a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주석처리된 코드는 이미지 저장을 위한 코드입니다.\n",
    "# 앞선 스켈레톤코드와 다를건없다.\n",
    "# pose모델에 bgr2rgb처리가 된 이미지를 process하여 results를 반환한다.\n",
    "# results.pose_landmarks를 통하여 랜드마크의 좌표를 가져올 수 있다.\n",
    "# 앞선 무브넷과 다르게 confidence는 앞선 코드인 mpPose.Pose()의 파라미터에서 수정가능하다.\n",
    "# drawing_utils로 간편하게 관절구조를 뽑아낼 수 있다는 장점이 있다.\n",
    "\n",
    "cap = cv2.VideoCapture('./Desktop/3.mp4')\n",
    "pTime=0 # for real time\n",
    "startTime=time.time()\n",
    "\n",
    "#for writing\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "codec='DIVX'\n",
    "fourcc=cv2.VideoWriter_fourcc(*codec)\n",
    "out = cv2.VideoWriter('output3.avi', fourcc, 30.0, (int(width), int(height)))\n",
    "\n",
    "\n",
    "# while nowTime-startTime<20:\n",
    "while 1:\n",
    "    ret, frame = cap.read()\n",
    "    nowTime=time.time()\n",
    "#     print(nowTime-startTime)\n",
    "    if ret:\n",
    "        rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        results = pose.process(rgb)\n",
    "        if results.pose_landmarks:\n",
    "            mpDraw.draw_landmarks(frame,results.pose_landmarks,mpPose.POSE_CONNECTIONS)\n",
    "        pTime=fps_detector(frame,pTime)\n",
    "        cv2.imshow('image',frame)\n",
    "        out.write(frame)\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6aefb8",
   "metadata": {},
   "source": [
    "### 예시영상 pose estimation 결과\n",
    "#### 무브넷과 다르게 관절이 부드럽고 랜드마크가 많아 더 정밀한 후처리가 가능하다는 점이 있다.\n",
    "#### 마찬가지로 single pose estimation이지만 한 사람을 추적하는것은 무브넷보다 뛰어나다\n",
    "#### 하지만 프레임은 확실히 떨어진다\n",
    "<img width=\"80%\" src=\"./img/output2.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a757590",
   "metadata": {},
   "source": [
    "### mediapipe Pose의 강점: segmentation_mask & layout effect\n",
    "##### 생각보다 강력한 기능인것 같다 무브넷보다 처리도 훨씬 간단하며 프레임드랍도 크지 않다\n",
    "##### 파라미터의 수정만으로 간단하게 mask를 추출할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89e9aa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4가지 parameters가 있지만 크게 영향을 주는 요소는\n",
    "# model_complexity와 enable_segmentation일것이다.\n",
    "# model_complexity는 말 그대로 모델의 복잡성을 설정하는것이 0~2(default:1)까지 가능하지만 높을수록 프레임드랍이 크다.\n",
    "# enable_segmentation은 results에 segmentation_mask 정보를 포함시킬지 여부이다. (default: False)\n",
    "\n",
    "# segmentation_mask추출을 위해 True로 정의하고 다시 코드를 진행시켜보겠다.\n",
    "pose_seg=mpPose.Pose(enable_segmentation=True)\n",
    "mpDraw=mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0a6d94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture('./Desktop/2.mp4')\n",
    "# cap = cv2.VideoCapture(0)\n",
    "pTime=0 # for real time\n",
    "startTime=time.time()\n",
    "nowTime=time.time()\n",
    "#for writing\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "codec='DIVX'\n",
    "fourcc=cv2.VideoWriter_fourcc(*codec)\n",
    "bg_img=None\n",
    "# out = cv2.VideoWriter('output3.avi', fourcc, 30.0, (int(width), int(height)))\n",
    "\n",
    "# while nowTime-startTime<20:\n",
    "while 1:\n",
    "    ret, frame = cap.read()\n",
    "    nowTime=time.time()\n",
    "#     print(nowTime-startTime)\n",
    "    if ret:\n",
    "        if bg_img is None:\n",
    "            bg_img=cv2.imread('./Desktop/bg2.jpg')\n",
    "            bg_img=cv2.resize(bg_img,(width,height))\n",
    "        rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "        results = pose_seg.process(rgb)\n",
    "        if results.segmentation_mask is not None:\n",
    "            condition= np.stack((results.segmentation_mask,)*3,axis=-1)>0.1\n",
    "            bg_image=np.zeros(frame.shape,dtype=np.uint8)\n",
    "            frame=np.where(condition,frame,bg_img)\n",
    "        \n",
    "        pTime=fps_detector(frame,pTime)\n",
    "        cv2.imshow('image',frame)\n",
    "#         out.write(frame)\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d1c36",
   "metadata": {},
   "source": [
    "### 예시영상  segmentation 결과\n",
    "##### single pose estimation의 segmentation확인을 위하여 다른 영상을 가져왔다.\n",
    "##### detecting이 이루어진 순간부터 정의된 background img로 대체되었다.\n",
    "##### 조금의 처리가 필요해 보이긴하지만 분명히 강력한 기능이다.\n",
    "\n",
    "<img width=\"80%\" src=\"./img/output3.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff687705",
   "metadata": {},
   "source": [
    "## (추가사항) [YOLOv3-tiny](https://github.com/pjreddie/darknet/blob/master/cfg/yolov3-tiny.cfg)\n",
    "#### 요약\n",
    "- 강력한 객체 탐지기능을 자랑하는 yolo의 yolov3의 라이트한 버전이다.\n",
    "- pose estimating에 특화된 모델은 아니지만 cpu환경에서도 객체탐지의 성능을 높이기위해 만들어진 모델이다\n",
    "- 만약 human detecting이 기존의 yolo만큼의 성능을 낼 수 있다면, 프레임마다 객체탐지로 human을 detecting하여 mediapipe pose를 적용시킨다면 multi pose estimating이 가능하지 않을까? 라는 생각에서 가져온 모델이다.\n",
    "- 결과적으로는 tiny라는 이름에 맞게 속도는 빨랐지만 class가 적고, weight도 작아서, 탐지성능이 떨어졌다. mediapipe multi pose detecting은 이루어질 수 없었다..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab22acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolov3-tiny weights, cfg를 받아와 경로를 입력한다.\n",
    "\n",
    "net = cv2.dnn.readNet(\"./Desktop/yolov3-tiny/weights/yolov3-tiny.weights\", \"./Desktop/yolov3-tiny/cfg/yolov3-tiny.cfg\")\n",
    "\n",
    "# 혹시나해서 기존 yolov3의 weights를 넣어봤지만 제대로 학습되지 않았다.. \n",
    "# net = cv2.dnn.readNet(\"./Desktop/yolov3-tiny/weights/yolov3.weights\", \"./Desktop/yolov3-tiny/cfg/yolov3-tiny.cfg\")\n",
    "\n",
    "classes = []\n",
    "with open(\"./Desktop/yolov3-tiny/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a446d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 객체를 탐지함과 동시에 결과값에서 \n",
    "# 탐지된 결과의 좌표값들을 추출해 cv2.rectangle, text로 객체를 레이블링한다\n",
    "\n",
    "cap = cv2.VideoCapture('./Desktop/6.mp4')\n",
    "# cap=cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "starting_time = time.time()\n",
    "frame_id = 0\n",
    "\n",
    "#for writing\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "codec='DIVX'\n",
    "fourcc=cv2.VideoWriter_fourcc(*codec)\n",
    "# out = cv2.VideoWriter('output6.avi', fourcc, 30.0, (int(width), int(height)))\n",
    "\n",
    "\n",
    "while 1:\n",
    "    ret,frame=cap.read()\n",
    "    if ret:\n",
    "        frame_id+=1\n",
    "        height, width, channels=frame.shape\n",
    "        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "        net.setInput(blob)\n",
    "        outs = net.forward(output_layers)\n",
    "        class_ids = []\n",
    "        confidences = []\n",
    "        boxes = []\n",
    "        for out in outs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "                if confidence > 0.1:\n",
    "                    # Object detected\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.4, 0.3)\n",
    "        for i in range(len(boxes)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "                color=(0,255,0)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + 30), color, -1)\n",
    "                cv2.putText(frame, label + \" \" + str(round(confidence, 2)), (x, y + 30), font, 3, (255,255,255), 3)\n",
    "        elapsed_time = time.time() - starting_time\n",
    "        fps = frame_id / elapsed_time\n",
    "        cv2.putText(frame, \"FPS: \" + str(round(fps, 2)), (10, 50), font, 3, (0, 0, 0), 3)\n",
    "        cv2.imshow(\"Image\", frame)\n",
    "#         out.write(frame)\n",
    "        if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "                break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "# out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32893f0f",
   "metadata": {},
   "source": [
    "### 예시영상  object detection 결과\n",
    "##### detecting이 빠르긴 하지만 정확도가 상당히 떨어지는 문제가 있다.\n",
    "##### 이러한 문제는 지속적인 pose estimating에 부정적인 영향을 줄 수 있기에 yolov3-tiny를 응용한 multi pose estimating은 활용되기 힘들것 같다.\n",
    "\n",
    "<img width=\"80%\" src=\"./img/output4.gif\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f681adfd",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e5c71",
   "metadata": {},
   "source": [
    "## 마치며..\n",
    "#### 기타 상세한 정확도와 시간등 여러 요건들이 이미 명세가 되어있는 pretrained모델을 가져와 학습을 진행했기에 수치적인 요소 보다는 cpu환경에서 얼마나 해당모델이 빠르고 정확하며, 주제에 적합한지 알아보기 위하여 여러 모델들을 적용시켜보았다.\n",
    "#### 적합한 모델을 찾기위해 꾸준히 학습하고 동향을 살펴보는 것도 좋은 방법이라고 생각하며, 여러가지 상황에 맞는 모델을 찾거나 학습시켜 웹서비스 프로젝트를 좋게 마무리짓고 싶다"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
